{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A stock trading environment for OpenAI gym\n",
    "    \"\"\"\n",
    "    def __init__(self, stock_data, initial_balance=10000, lookback_window_size=10):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        self.stock_data = stock_data\n",
    "        self.initial_balance = initial_balance\n",
    "        self.lookback_window_size = lookback_window_size\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)  # Sell, Hold, Buy\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(lookback_window_size + X,))  # +X for additional indicators\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.current_step = 0\n",
    "        self.holdings = 0  # Number of shares held\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        # Use data for the last N days\n",
    "        frame = self.stock_data.iloc[self.current_step:self.current_step + self.lookback_window_size]\n",
    "\n",
    "        # Append additional features like balance, holdings and technical indicators\n",
    "        additional_features = self._get_technical_indicators(frame)\n",
    "        obs = np.append(frame.values.flatten(), [self.balance, self.holdings, *additional_features])\n",
    "        return obs\n",
    "\n",
    "    def _get_technical_indicators(self, frame):\n",
    "        # Calculate and return technical indicators (like moving average, RSI, etc.)\n",
    "        # Dummy implementation\n",
    "        return [0.0] * 20  # Replace X with the number of indicators\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.stock_data.iloc[self.current_step]['Close']\n",
    "        self.current_step += 1\n",
    "\n",
    "        if action == 0:  # Sell\n",
    "            self.balance += self.holdings * current_price\n",
    "            self.holdings = 0\n",
    "        elif action == 2 and self.balance >= current_price:  # Buy\n",
    "            self.holdings += self.balance // current_price\n",
    "            self.balance -= self.holdings * current_price\n",
    "\n",
    "        done = self.current_step >= len(self.stock_data)\n",
    "        reward = self._calculate_reward()\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        # Simple reward: Change in portfolio value\n",
    "        current_portfolio_value = self.balance + self.holdings * self.stock_data.iloc[self.current_step]['Close']\n",
    "        return current_portfolio_value - self.initial_balance\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        # Optionally implement rendering for visualization\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "policy_net = PolicyNetwork(input_size=30, hidden_size=128, output_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: MAML Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "\n",
    "def inner_loop(policy, optimizer, env, num_steps):\n",
    "    for _ in range(num_steps):\n",
    "        states, actions, rewards = sample_trajectory(env, policy)\n",
    "        returns = compute_returns(rewards)\n",
    "        policy_loss = compute_policy_loss(states, actions, returns, policy)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def maml_update(initial_policy, tasks, alpha, beta, num_inner_steps):\n",
    "    meta_policy = deepcopy(initial_policy)\n",
    "    meta_optimizer = optim.Adam(meta_policy.parameters(), lr=beta)\n",
    "\n",
    "    task_grads = []  # List to store gradients for each task\n",
    "\n",
    "    for task_env in tasks:\n",
    "        task_policy = deepcopy(meta_policy)\n",
    "        task_optimizer = optim.Adam(task_policy.parameters(), lr=alpha)\n",
    "        \n",
    "        # Perform task-specific updates\n",
    "        inner_loop(task_policy, task_optimizer, task_env, num_inner_steps)\n",
    "\n",
    "        # Compute gradients for task\n",
    "        for param, task_param in zip(meta_policy.parameters(), task_policy.parameters()):\n",
    "            if task_param.grad is not None:\n",
    "                # Accumulate gradients from this task\n",
    "                if param.grad is None:\n",
    "                    param.grad = torch.zeros_like(param.data)\n",
    "                param.grad.data.add_(task_param.data - param.data)\n",
    "\n",
    "        task_grads.append([param.grad for param in meta_policy.parameters()])\n",
    "\n",
    "    # Aggregate gradients across tasks and apply meta-update\n",
    "    for param_index, param in enumerate(meta_policy.parameters()):\n",
    "        meta_grad = torch.stack([grads[param_index] for grads in task_grads]).mean(dim=0)\n",
    "        param.data.sub_(meta_grad.data * beta)  # Update the initial policy\n",
    "\n",
    "    return meta_policy\n",
    "\n",
    "\n",
    "    # Meta-update logic\n",
    "    # ...\n",
    "\n",
    "def sample_trajectory(env, policy, max_trajectory_length=1000):\n",
    "    states, actions, rewards = [], [], []\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(max_trajectory_length):\n",
    "        state_tensor = torch.from_numpy(state).float()\n",
    "        action_prob = policy(state_tensor)\n",
    "        action = torch.multinomial(action_prob, 1).item()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards\n",
    "\n",
    "\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "\n",
    "def compute_policy_loss(states, actions, returns, policy):\n",
    "    loss = 0\n",
    "    for state, action, R in zip(states, actions, returns):\n",
    "        state_tensor = torch.from_numpy(state).float()\n",
    "        action_probs = policy(state_tensor)\n",
    "        action_prob = action_probs[action]\n",
    "        loss -= torch.log(action_prob) * R  # Negative because we want to maximize\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other needed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def step(self, action):\n",
    "    current_price = self.stock_data[self.current_step]['Close']\n",
    "    self.current_step += 1\n",
    "\n",
    "    if action == 0:  # Sell\n",
    "        # Implement selling logic: update balance based on current_price\n",
    "        pass\n",
    "    elif action == 2:  # Buy\n",
    "        # Implement buying logic: update balance based on current_price\n",
    "        pass\n",
    "    # Hold does nothing\n",
    "\n",
    "    reward = self._calculate_reward()\n",
    "    done = self.current_step >= len(self.stock_data)\n",
    "    return self._next_observation(), reward, done, {}\n",
    "\n",
    "def sample_tasks(stock_data, num_tasks):\n",
    "    sampled_stocks = random.sample(list(stock_data.keys()), num_tasks)\n",
    "    return [StockTradingEnv(stock_data[stock]) for stock in sampled_stocks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 : Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_iterations = 100\n",
    "num_inner_steps = 5\n",
    "alpha = 0.01  # Inner loop learning rate\n",
    "beta = 0.001  # Meta learning rate\n",
    "\n",
    "# Policy network setup\n",
    "input_size = 20  # Define according to your state representation\n",
    "hidden_size = 64\n",
    "output_size = 3  # Buy, hold, sell\n",
    "\n",
    "initial_policy = PolicyNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Main training loop\n",
    "for iteration in range(num_iterations):\n",
    "    tasks = sample_tasks(stock_list, num_tasks_per_iteration)  # Sample tasks\n",
    "\n",
    "    initial_policy = maml_update(initial_policy, tasks, alpha, beta, num_inner_steps)\n",
    "    # Evaluate the updated policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Evaluate the updated policy on validation tasks\n",
    "    \n",
    "# ... rest of the functions used above\n",
    "def initialize_policy_parameters():\n",
    "    # Assuming a policy network class has been defined\n",
    "    policy_net = PolicyNetwork(input_size, hidden_size, output_size)\n",
    "    return policy_net.parameters()\n",
    "def sample_trajectories(task_env, policy_params, num_trajectories=K):\n",
    "    trajectories = []\n",
    "    policy_net.load_state_dict(policy_params)  # Assuming policy_net is a global variable for the policy network\n",
    "    \n",
    "    for _ in range(num_trajectories):\n",
    "        state = task_env.reset()\n",
    "        trajectory = []\n",
    "        while not done:\n",
    "            action = policy_net.act(state)  # Assuming a method that decides an action based on the current state\n",
    "            next_state, reward, done, _ = task_env.step(action)\n",
    "            trajectory.append((state, action, reward))\n",
    "            state = next_state\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories\n",
    "def compute_loss(trajectories, policy_params):\n",
    "    policy_losses = []\n",
    "    policy_net.load_state_dict(policy_params)  # Load the current policy parameters\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "        for state, action, reward in trajectory:\n",
    "            # Compute the policy loss from the trajectory data\n",
    "            # This could use a function like policy_net.evaluate_actions\n",
    "            log_prob = policy_net.evaluate_actions(state, action)\n",
    "            policy_losses.append(-log_prob * reward)  # Negative because we perform gradient ascent\n",
    "    return torch.stack(policy_losses).sum()\n",
    "def compute_meta_gradient(tasks, D_prime, theta, theta_i_prime, task_gradients):\n",
    "    meta_gradient = torch.zeros_like(theta)\n",
    "    \n",
    "    for i, task in enumerate(tasks):\n",
    "        # Load the adapted parameters for the current task\n",
    "        policy_net.load_state_dict(theta_i_prime[i])\n",
    "        \n",
    "        # Sample new trajectories using the adapted parameters\n",
    "        new_trajectories = sample_trajectories(task, theta_i_prime[i], num_trajectories=K)\n",
    "        \n",
    "        # Compute the loss with the new trajectories\n",
    "        new_loss = compute_loss(new_trajectories, theta_i_prime[i])\n",
    "        \n",
    "        # Compute the gradient w.r.t. the adapted parameters\n",
    "        adapted_gradients = torch.autograd.grad(new_loss, theta_i_prime[i])\n",
    "        \n",
    "        # Combine the adapted gradient with the initial gradient for the meta-gradient\n",
    "        meta_gradient += adapted_gradients[0] - task_gradients[i]\n",
    "    \n",
    "    return meta_gradient / len(tasks)\n",
    "\n",
    "# Initialization\n",
    "theta = initialize_policy_parameters()\n",
    "meta_optimizer = torch.optim.Adam([theta], lr=beta)\n",
    "\n",
    "# Meta-training loop\n",
    "while not done:\n",
    "    task_gradients = []  # To store gradients for each task\n",
    "    \n",
    "    # Task sampling\n",
    "    tasks = sample_tasks(num_tasks)\n",
    "    \n",
    "    # Inner loop\n",
    "    for task in tasks:\n",
    "        # Copy initial policy parameters for task-specific learning\n",
    "        theta_i = deepcopy(theta)\n",
    "        \n",
    "        # Sample K trajectories using the current policy parameters theta\n",
    "        D = sample_trajectories(task, theta_i, num_trajectories=K)\n",
    "        \n",
    "        # Evaluate gradients of the loss w.r.t. theta_i\n",
    "        task_loss = compute_loss(D, theta_i)\n",
    "        task_gradients.append(torch.autograd.grad(task_loss, theta_i))\n",
    "        \n",
    "        # Compute adapted parameters theta'_i using gradient descent\n",
    "        theta_i_prime = theta_i - alpha * task_gradients[-1]\n",
    "        \n",
    "        # Sample new trajectories D'_i using the adapted parameters theta'_i\n",
    "        D_prime = sample_trajectories(task, theta_i_prime, num_trajectories=K)\n",
    "    \n",
    "    # Outer loop\n",
    "    meta_gradient = compute_meta_gradient(tasks, D_prime, theta, theta_i_prime, task_gradients)\n",
    "    meta_optimizer.zero_grad()\n",
    "    theta.grad = meta_gradient\n",
    "    meta_optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
